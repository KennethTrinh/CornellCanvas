<h1>
 Title: A cool dataset source
</h1>
<h3>
 Author: Celine Lee (student)
</h3>
<h3>
 Date: 2021-10-09T06:16:09.443515+11:00
</h3>
<h3>
 Category: Suggestions (related to A2)
</h3>
<h3>
 Vote Count: 8
</h3>
<p>
 I've been looking around at potential dataset sources, and I think this one bears a striking resemblance to the AI datasets, but I have been poking around at it and it also seems really rich and interesting.  I recall a few comments/conversations about Stack Exchange yesterday during class, so I thought I'd share it with the class in case it hasn't been shared already.
</p>
<p>
 The Internet Archive has these periodic data dumps from Stack Exchange (years 2014-2021). If you click on 'Show all files', you will see a bunch of
 <code>
  .7z
 </code>
 files corresponding to different Stack Exchange communities (e.g. astronomy, beer, cs, economics... it's a long list).
</p>
<p>
 <a href="https://archive.org/details/stackexchange">
  https://archive.org/details/stackexchange
 </a>
</p>
<p>
 The readme that explains the files of each
 <code>
  .7z
 </code>
 file is here:
</p>
<p>
 <link href="https://ia800107.us.archive.org/27/items/stackexchange/readme.txt"/>
 https://ia800107.us.archive.org/27/items/stackexchange/readme.txt
</p>
<p>
 It looks like you can get:
</p>
<list style="bullet">
 <li>
  <p>
   user data (reputation, id, activity, etc.)
  </p>
 </li>
 <li>
  <p>
   vote data (vote type, user, etc.)
  </p>
 </li>
 <li>
  <p>
   post edit history (edits, rollbacks, votes to close/reopen/delete/etc., etc.)
  </p>
 </li>
 <li>
  <p>
   post links (linked or duplicate)
  </p>
 </li>
 <li>
  <p>
   post (tag, user poster, score, text, title, etc.)
  </p>
 </li>
 <li>
  <p>
   comment (parent post, score, text, etc.)
  </p>
 </li>
</list>
<p>
 + some more information.
</p>
<p>
 Hope this might be helpful!
</p>
<div style="text-indent: 2em;">
 <h3>
  Author: Kiran Tomlinson (student)
 </h3>
 <h3>
  Vote Count: 3
 </h3>
 <p>
  I used this data recently for a research project--it is great. However, the archive.org downloads were **very** slow. I have all the data (76GB compressed) downloaded on my desktop in Gates. If anyone wants to use the data, I'd be happy to help them transfer it from there (which will be much faster). I'd also be happy to share my parsing code, although I only looked at the user-question interaction network (who replied to/commented on every question).
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Celine Lee (student)
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   Awesome, thanks Kiran! Iâ€™d be curious to check out your parsing code, if you are happy to share. :)
  </p>
  <h3>
   ------------------------------------
  </h3>
  <div style="text-indent: 6em;">
   <h3>
    Author: Kiran Tomlinson (student)
   </h3>
   <h3>
    Vote Count: 0
   </h3>
   <p>
    Yes! Sorry for the delay. Here's the code I used to preprocess the user-question networks:
   </p>
   <snippet language="py" line-numbers="true" runnable="false">
    <snippet-file id="code">
     from lxml import etree
from py7zr import SevenZipFile
import pandas as pd


def get_post_df(posts):
    question_times = {}
    answers = []

    # Id, PostTypeId (1 = question, 2 = answer, ignore others), ParentId (if PostTypeId==2),
    # CreationDate, OwnerUserId (absent if user deleted)
    for post in posts:

        post_type = post.get('PostTypeId')

        if post_type == '1':
            id = post.get('Id')
            creation_date = post.get('CreationDate')

            question_times[id] = creation_date
        elif post_type == '2':
            if 'OwnerUserId' not in post.attrib:
                continue

            owner = post.get('OwnerUserId')
            creation_date = post.get('CreationDate')
            parent_id = post.get('ParentId')

            answers.append((owner, parent_id, creation_date))

    df = pd.DataFrame(answers, columns=['user', 'post', 'comment_time'])
    df['comment_time'] = pd.to_datetime(df['comment_time'])
    df['post_time'] = pd.to_datetime(df['post'].map(question_times))


    assert not df.isnull().values.any()

    return df


def preprocess_stack_exchange():
    parser = etree.XMLParser(huge_tree=True)
    for file in glob.glob(f'stack-exchange/stackexchange/*.7z'):

        # Handle stackoverflow later
        if 'stackoverflow.com-' in file:
            print('Skipping', file)
            continue

        print('Processing', file)
        with SevenZipFile(file, 'r') as f:
            # files = f.read(['Posts.xml', 'Comments.xml'])
            posts = etree.parse(f.read(['Posts.xml'])['Posts.xml'], parser=parser).getroot()
            # comments = etree.parse(files['Comments.xml'], parser=parser).getroot()

        df = get_post_df(posts)

        site_name = os.path.basename(file).replace('.7z', '')
        df.to_csv(f'{RAW_DATA_DIR}/stack-exchange/{site_name}-df.csv', index=False)

    print('Processing stackoverflow')

    # Warning: stackoverflow needs A LOT of ram (probably &gt; 100GB)
    with SevenZipFile(f'{RAW_DATA_DIR}/stack-exchange/stackexchange/stackoverflow.com-Posts.7z', 'r') as f:
        posts = etree.parse(f.read(['Posts.xml'])['Posts.xml'], parser=parser).getroot()

    # with SevenZipFile(f'{RAW_DATA_DIR}/stack-exchange/stackexchange/stackoverflow.com-Comments.7z', 'r') as f:
    #     comments = etree.parse(f.read(['Comments.xml'])['Comments.xml'], parser=parser).getroot()

    df = get_post_df(posts)
    df.to_csv(f'stack-exchange/stackoverflow.com-df.csv', index=False)
    </snippet-file>
   </snippet>
   <h3>
    ------------------------------------
   </h3>
  </div>
 </div>
</div>
<h3>
 ----------- REPLIES -----------
</h3>