<h1>
 Title: Progress Report for Politeness, Emotion and Small talks
</h1>
<h3>
 Author: Fengyang Lin (student)
</h3>
<h3>
 Date: 2021-12-03T03:16:46.438414+11:00
</h3>
<h3>
 Category: Progress report for in-class comments (A5)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <heading level="1">
  Project Overview
 </heading>
 <paragraph>
  For this final project, I aim to focus on politeness, emotion and small talks during the persuasion task for donation, investigating their impacts and correlations on the success of the persuasion and the amount of the donation. The hypothesis I proposed are as follow:
 </paragraph>
 <list style="unordered">
  <list-item>
   <paragraph>
    H1: There is a significant difference in the extent of politeness of persuaders among successful conversations and the unsuccessful ones. However, as long as the conversation is successful - the persuadee agreed to make a donation, the politeness level of persuaders does not vary much with respect to the donation amount.
   </paragraph>
  </list-item>
  <list-item>
   <paragraph>
    H2: A more polite persuadee is not necessarily more likely to donate (and donate more) to the charity program. Instead, for those who apologized during the conversations, they refused to donate or donate less.
   </paragraph>
  </list-item>
  <list-item>
   <paragraph>
    <italic>
     [Dropped] H3: A conversation with stronger emotions is more likely to succeed and people are willing to give more during and after the conversations.
    </italic>
   </paragraph>
  </list-item>
  <list-item>
   <paragraph>
    H4: Having small talks at the beginning of the conversation, even off-topic, could increase the likelihood of persuadees’ donation intention and behavior.
   </paragraph>
  </list-item>
 </list>
 <paragraph>
  The dataset I am working on is
  <italic>
   <bold>
    Persuasion-ForGood
   </bold>
  </italic>
  dataset, which is available on Convokit (
  <link href="https://convokit.cornell.edu/documentation/persuasionforgood.html"/>
  https://convokit.cornell.edu/documentation/persuasionforgood.html). This dataset contains
  <bold>
   1017 conversations
  </bold>
  , along with demographic data and responses to psychological surveys from users. 300 conversations also have per-sentence human annotations of dialogue acts that pertain to the persuasion setting, and sentiment.
 </paragraph>
 <paragraph>
  Note that given the result of the feasibility test, the accuracy of VADER sentiment analysis was not ideal on the dataset. Additionally, I found it a little bit tricky to set up the standard of different sentiment labels. Therefore I decided to drop the Emotion part for now. The next part will report my progress and results for politeness and small talk.
 </paragraph>
 <heading level="1">
  Politeness
 </heading>
 <heading level="2">
  Method
 </heading>
 <paragraph>
  <bold>
   Generate politeness label/level
  </bold>
 </paragraph>
 <paragraph>
  I applied the politeness prediction tool in Convokit to generate the politeness strategy markers, probability, and labels. To predict the politeness label/level of a given utterance, I trained the default model in Convokit with the labeled Wiki Corpus. The default threshold for politeness was 0.5, and I adjusted the threshold to 0.7 based on the results of the 50 utterances I labeled on this dataset. As a reference, the median of the politeness probability/level of the PFG dataset generated by the classifier is 0.47 and the mean is 0.46.
 </paragraph>
 <paragraph>
  <bold>
   From utterance to conversation
  </bold>
 </paragraph>
 <paragraph>
  At the moment, to obtain the politeness level of a persuader/persuadee in a given conversation, I first generate the politeness label for each utterance based on the 0.7 thresholds, then calculate
  <italic>
   [num of polite utterances]/[num of all utterances]
  </italic>
  as the indicator for the politeness level of the speaker in the conversation. If the politeness level is greater than 0.3, I will take the speaker as a polite one.
 </paragraph>
 <paragraph>
  <bold>
   Q:
  </bold>
  Is this a good way to calculate the politeness level of a speaker? Would it be better if I just average the politeness probability rather than relying on the proportion of the polite utterances? (Also what is a good threshold?)
 </paragraph>
 <heading level="2">
  Result
 </heading>
 <list style="unordered">
  <list-item>
   <paragraph>
    H1: There is a significant difference in the extent of politeness of persuaders among successful conversations and the unsuccessful ones. However, as long as the conversation is successful - the persuadee agreed to make a donation, the politeness level of persuaders does not vary much with respect to the donation amount.
   </paragraph>
  </list-item>
 </list>
 <figure>
  <image height="108" src="https://static.us.edusercontent.com/files/Bfk0jHblKzuv1vFKvOPskr2s" width="270"/>
 </figure>
 <figure>
  <image height="105" src="https://static.us.edusercontent.com/files/bZ8QtD2dSeEJ6I2Qf08MDF0h" width="260"/>
 </figure>
 <paragraph>
  Given the 2x2 table, I conducted Chi-square tests for both the utterance level and conversation level. (In fact, I believe that only the results for the conversation level are meaningful.) The p-value for the utterance level is 0.000395, the one for the conversation level is 0.001288. We could claim that politeness and the success of persuasion are not independent. Still, I am considering using a different one-way test (
  <bold>
   one-way Chi-square test
  </bold>
  ) to maintain a more convincing result.
 </paragraph>
 <figure>
  <image height="262" src="https://static.us.edusercontent.com/files/Se623KvA8wcKCewaYTmydSyl" width="389"/>
 </figure>
 <list style="unordered">
  <list-item>
   <paragraph>
    H2: A more polite persuadee is not necessarily more likely to donate (and donate more) to the charity program. Instead, for those who apologized during the conversations, they refused to donate or donate less.
   </paragraph>
  </list-item>
 </list>
 <figure>
  <image height="102" src="https://static.us.edusercontent.com/files/8HbIdD7Rr6EOz2xAhieC2UgL" width="250"/>
 </figure>
 <figure>
  <image height="97" src="https://static.us.edusercontent.com/files/FDs6HGzVEW2IJzXh1JzIU0rE" width="257"/>
 </figure>
 <paragraph>
  Similar to persuaders, I conducted Chi-square tests for both the utterance level and conversation level. The p-value for the utterance level is 0.103379, the one for the conversation level is 0.4636. It seems like that the politeness level of the persuadees would not indicate whether he/she will donate or not. (TODO: One-way test)
 </paragraph>
 <figure>
  <image height="262" src="https://static.us.edusercontent.com/files/fIJK39dG4715VcAGolGzpeCr" width="397"/>
 </figure>
 <paragraph>
  One of the next steps is to generate a linear/logistic model with the politeness strategy makers. I think it would be interesting to see which of the strategy would have a great impact on the donation amount. The figures below demonstrate a quick check of the markers.
 </paragraph>
 <paragraph>
  For successful persuader:
 </paragraph>
 <figure>
  <image height="309" src="https://static.us.edusercontent.com/files/nPAuiFyjg6czsf3sgvUGha5h" width="410"/>
 </figure>
 <paragraph>
  For unsuccessful persuader:
 </paragraph>
 <figure>
  <image height="309" src="https://static.us.edusercontent.com/files/7SK0s6q4zL3v50aua6kxOf7x" width="410"/>
 </figure>
 <heading level="1">
  Small talks
 </heading>
 <list style="unordered">
  <list-item>
   <paragraph>
    H4: Having small talks at the beginning of the conversation, even off-topic, could increase the likelihood of persuadees’ donation intention and behavior.
   </paragraph>
  </list-item>
 </list>
 <heading level="2">
  Method
 </heading>
 <paragraph>
  For now, I decide to use the Naive Bayes' approach to classify whether an utterance is small talk or not.
 </paragraph>
 <math>
  P\left(Class,\ Utt\right)=P\left(Class,\ w_1,\ w_2,\ ...,\ w_n\right)
 </math>
 <math>
  \left(=P\left(w_n\right|w_1,...w_{n-1},Class\right)\cdot P\left(w_1,...w_{n-1},Class\right)
 </math>
 <paragraph>
  Using Markov Assumption (Bi-gram), the formula could be developed into
 </paragraph>
 <math>
  \left(\left(\left(P\left(w_n\right|w_{n-1},C\right)P\left(w_{n-1}\right|w_{n-2},C\right)...P\left(w1\right|start,\ C\right)
 </math>
 <math>
  \left(=\prod_{ }^{ }P\left(w_i\right|w_{i-1},C\right)
 </math>
 <list style="bullet">
 </list>
 <paragraph>
  The workflow should be as follow:
 </paragraph>
 <list style="bullet">
  <list-item>
   <paragraph>
    Label utterances into class either small talk or not small talk
   </paragraph>
   <list style="bullet">
    <list-item>
     <paragraph>
      How many labeled utterances should be enough?
     </paragraph>
    </list-item>
   </list>
  </list-item>
  <list-item>
   <paragraph>
    Generate probability estimate for P(w_i|w_{i-1},C) based on my labeled dataset + annotated labeled set by the publisher (they labeled utterance with “personal-related inquiry”, “personal story”, “greeting”, “off-tasks”.)
   </paragraph>
   <list style="bullet">
    <list-item>
     <paragraph>
      Deal with words and bi-gram pairs that did not appear in the label set.
     </paragraph>
    </list-item>
   </list>
  </list-item>
  <list-item>
   <paragraph>
    Calculate P(st, utterance) and P(nst, utterance) for each utterance
   </paragraph>
   <list style="bullet">
    <list-item>
     <paragraph>
      if P(st, utterance) &gt;= P(nst, utterance), predict the utterance is small talk.
     </paragraph>
    </list-item>
   </list>
  </list-item>
  <list-item>
   <paragraph>
    Generate an indicator of whether small talks happened in the conversation
   </paragraph>
  </list-item>
 </list>
 <paragraph>
  <bold>
   Q:
  </bold>
  Is there any better language model?
 </paragraph>
 <list style="bullet">
  <list-item>
   <list style="bullet">
    <list-item>
     <paragraph>
      Consider models that class/state for different words in an utterance could be different (as in the in-class example) ...
     </paragraph>
    </list-item>
    <list-item>
     <paragraph>
      Do not take words order into account in the model?
     </paragraph>
    </list-item>
   </list>
  </list-item>
 </list>
</document>
<h3>
 Author: Kiran Tomlinson (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  This is a really cool project! The result that persuader (but not persuadee) politeness is significantly correlated with donations is very nice and intuitive! It would be helpful to have the 2x2 tables in proportions rather than counts, since it's hard to tell at a glance if the fraction of successful utterances that are polite is larger than the fractions of unsuccessful utterances that are polite. r^2 values for the linear regressions on donation amount would also provide numerical backing to the claim that donation amounts aren't affected by politeness (which definitely seems to be the case).
 </paragraph>
 <paragraph>
  I'm not sure than performing a one-way Chi-square test is justified, given that you already have significance in a two-way test (while it might have a smaller p-value, it wouldn't allow you to reject the hypothesis that the true rates differ in the other direction).
 </paragraph>
 <paragraph>
  I think taking the average politeness score as you suggested could be a nice next step rather than binning into polite and impolite--then you can compare the two distributions of mean politeness (for successful and unsuccessful conversations) with a t-test.
 </paragraph>
 <paragraph>
  The naive Bayesian approach to identifying small talk makes sense to me. My only concern is that you might need to label a lot of utterances (although I'm not sure how many are required). To make things easier for yourself, maybe you could use the provided "greeting" label as an indicator for small talk?
 </paragraph>
 <paragraph>
  One last thing: Linda and I are also working with politeness, but we haven't seen a single-value estimate for politeness--could you link to the ConvoKit documentation?
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Fengyang Lin (student)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  Just a note from our discussion - the link for the single-value estimate of politeness I used is the prediction score from the trained classifier. You could find it here in cell 24/25:
  <link href="https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/politeness-strategies/politeness_demo.ipynb"/>
  https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/politeness-strategies/politeness_demo.ipynb Note that the default threshold of the toolkit here for generating the politeness label is 0.5.
 </paragraph>
 <paragraph>
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Fengyang Lin (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Notes from today's discussion for small talks:
 </paragraph>
 <list style="bullet">
  <list-item>
   <paragraph>
    Consider using some simple models as naive Bayes or BOW as the baseline model.
   </paragraph>
  </list-item>
  <list-item>
   <paragraph>
    Consider using existing word-embedding methods -&gt; linear/logistic/NN/... model -&gt; train a classifier
   </paragraph>
   <list style="bullet">
    <list-item>
     <paragraph>
      If could not find an extra dataset for small talk, look for a dataset of a similar task
     </paragraph>
    </list-item>
   </list>
  </list-item>
 </list>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Remy Stewart (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Hi there Fengyang, I was working on feedback for your project during the last part of class and wanted to make sure I sent it over to you in case you may find it helpful! I really like how unique and relevant your project idea is, and I think it’s tackling a language context around politeness and small talk that really could have a sizable impact on an outcome like charitable donations. I’m very excited to see you continue to build from these preliminary results and I’m certain you’re going to have quite the strong final submission for this course.
 </paragraph>
 <paragraph>
  I first wanted to engage with the questions you pose at the end of your methods section. Although I’m not entirely following what the “threshold” metric is exactly measuring since I’m not too familiar with the Convokit prediction tool itself, I think maybe running some experiments to see if your substantive results change by using different thresholds could be helpful for testing out the impact of the threshold divide. That would serve as a helpful check to either the robustness of your findings to potential threshold choices, whether by highlighting that the core takeaways stay the same or interpreting why you believe you may see substantive differences if the results do change.
 </paragraph>
 <paragraph>
  I noticed in your second hypothesis you have an additional interest in the role of apologies on less potential donations. I may have missed it in my reading of your results so far, but were you able to test out that component of your research question yet? I can see from your categorical histogram that the apologizing term by utterance seems to be quite small, so it may be more a matter of limited occurrences in the data than anything.
 </paragraph>
 <paragraph>
  I know that I’ve encountered similar problems with wanting to run a Naïve Bayes model with a limited number of labeled instances. I think I tested around 350 records for a binary text classification task once with NB and really didn’t get that great of results, but a regularized logistic regression significantly improved my accuracy instead. I think the note you made in the previous comments about using pre-trained embeddings could be a solid approach for handling this challenge since you’ll already have core context dimensions available which can likely reduce the number of instances you’ll have to label to build a solid model.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Fengyang Lin (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Hi Remy, thank you for the kind reply and your suggestions!!
 </paragraph>
 <paragraph>
  As I understand it, the probability/threshold from the Convokit tool refers to how the trained classifier is confident about the utterance is polite or not -  so I was playing with the threshold to say if the prob/confidence from the classifier is greater than 0.7, I will treat this utterance as a polite one. And yes, I will definitely tune the threshold and check the impact with a small set of labeled data!
 </paragraph>
 <paragraph>
  In response to the second hypothesis for the role of apologies on less potential donations, I have to admit that carrying out a test for conversations with apologies is my next step, so I did not present any results yet. I was also worried about the number of conversations might be limited, but I was also considering using these politeness markers(including apology) to build a logistic model, so maybe we could also gain some insights from the model results then.
 </paragraph>
 <paragraph>
  As for running the NB model, I feel the same way that it is really hard to have "acceptable" performance with a limited number of labeled data as the model is still too simple. Thank you for your suggestions on using a regularized logistic regression! I was also thinking maybe I could introduce some self-defined features/0-1 indicators into the models in hope of having a more accurate model/classifier. Maybe I am too naive at saying this - as long as we are treating it as a classfication problem, I guess we don't need to limit ourselves with those "laungauge model features"? Will keep it update!
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Prof. Lee (she/they) (admin)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Small presentation request: could you plot the histogram for word categories of successful persuaders and word categories of unsuccessful persuaders on the same plot?  It looks like the two histograms have similar overall "shapes" but the values for unsuccessful persuaders are just lower; I'd like to see if this seems to be true.
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Fengyang Lin (student)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  Apologies for the radio silence! Yes, for most politeness strategy features, successful persuaders have a higher value as shown in the below plot.
 </paragraph>
 <figure>
  <image height="442.9066366704162" src="https://static.us.edusercontent.com/files/jTppj65LK9nbjfthW8QURcvN" width="624"/>
 </figure>
 <paragraph>
  I also plotted the one for persuadee, and seems like that the cases are mixed. Additionally, it is true that nondonated persuadees apologize more than the donated ones, but as both values for occurrence are small so it might be a biased statement.
 </paragraph>
 <figure>
  <image height="432.25984251968504" src="https://static.us.edusercontent.com/files/4NWa1ohVuz5HmbFh8JZgHsjR" width="609"/>
 </figure>
 <paragraph>
  P.S. Values of the average occurrences per utterance in these bar plots are slightly different from the ones in the post. The ones in the post are generated with Convikit functions, and the compared ones here are plotted with my own codes. I haven't really figured out why the "magnitude of values" are different among the two "functions", but as the shapes are the same I just stay with the current results.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 ----------- REPLIES -----------
</h3>