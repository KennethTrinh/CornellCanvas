<h1>
 Title: Personal Experiences vs. Opinions A1 Pilot Study- Reflection Post
</h1>
<h3>
 Author: Remy Stewart (student)
</h3>
<h3>
 Date: 2021-09-22T05:09:00.511654+10:00
</h3>
<h3>
 Category: A1_reflection
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Here are my slides and project submission in preparation for the reflection! :)
</p>
<a filename="Remy's A1 Project Report.pdf" href="https://static.us.edusercontent.com/files/p1jDoCJdfgNxsAZIDaIyjUON">
</a>
<file url="https://static.us.edusercontent.com/files/ovCZeaPIhMa04A1qSkebcNkY">
 Remy's A1 Presentation.pptx
 <div style="text-indent: 2em;">
  <h3>
   Author: Khonzoda Umarova (she/her) (student)
  </h3>
  <h3>
   Vote Count: 1
  </h3>
  <p>
   Hi Remy,
  </p>
  <p>
   I really enjoyed your presentation! Looking at persuasiveness of past experiences vs. beliefs is fascinating and complex, as one can find explanations why using each of these as part of an argument could potentially be convincing. Most of all I was impressed by your technique for generating vocabulary lists for experience and belief verbs. While they might not be exhaustive, I found these lists to be a smart way to operationalize experience and belief language in our preliminary study.
  </p>
  <p>
   Your presentation made me think of whether it is possible that past experiences are used as a support mechanism for personal beliefs expressed in the post. So, something that I would like to start with if I were to join your team is reading closely posts that seem to have both experience and belief verbs.
  </p>
  <p>
   Something else that I would like to experiment with is pairing. In order to account for potential differences in topics between posts, I wonder if it is possible to establish pairs within the same OP. So first, we may try to define a metric for the extent to which a post contains experience vs belief language by looking at the proportion of sentences with each kinds of verbs. Then for every conversation, apply the metric to all replies under the OP and select two that are farthest apart (i.e. one that mostly contains beliefs vs. one that mostly references experiences) with a chosen threshold of the difference. I am hoping that using such pairs might help with some confounding factors/variables with regards to the OP and conversation topic.
  </p>
  <h3>
   ------------------------------------
  </h3>
  <div style="text-indent: 2em;">
   <h3>
    Author: Linda Wang (student)
   </h3>
   <h3>
    Vote Count: 1
   </h3>
   <p>
    Hi Remy, Thanks again for sharing your project with the class! I really appreciate how your measures of “experience” versus “belief” were both intuitive and falsifiable across (what I imagine to be) the huge range of topics covered in the dataset. I also found your idea to incorporate ambiguous words into a broadly-defined “experience” dictionary to be quite interesting. If I understood correctly, it seems you implicitly assumed that, when faced with an ambiguous phrasing, viewers have a tendency to treat it more as an “experience” phrasing than as a “belief” phrasing. This assumption is novel to me, and it may be an intriguing area of study in and of itself.
    <break>
    </break>
   </p>
   <p>
    Your project has a lot of potential future directions! I wonder whether it would be beneficial to see whether the topic of a given CMV thread affects the persuasiveness of “experience”- versus “belief”-based comments? Despite the guidelines that CMV lays out, it seems plausible that polarizing topics draw more emotional or stronger language than do other topics, at least unconsciously; this context may change the impact that “experience”- or “belief”-based comments have on the OP. Conditioning your counts on thread topics may then give you different descriptive statistics and or regression results. Practically speaking, this would likely require some topic modeling beforehand to set up your conditions.
   </p>
   <h3>
    ------------------------------------
   </h3>
   <div style="text-indent: 2em;">
    <h3>
     Author: Heather Zheng (student)
    </h3>
    <h3>
     Vote Count: 1
    </h3>
    <p>
     Hi Remy, I think learning how personal experiences versus personal beliefs impact persuasiveness is a very meaningful topic. It’s very complex research, and I pretty like the way you make some assumptions to simplify the approach. Also, it’s very impressive that you identify the verb belongs to personal experience or belief, and also differentiate between core experience terms and ambiguous experience terms.
    </p>
    <p>
     Although this pilot study indicates that there’s no association between the use of personal experience or belief terms. I think for future studies, it would be interesting to see if the proportion of personal experiences and personal beliefs in the replies influence persuasiveness. Possible steps are: Identify if the sentence has more experience or personal beliefs. Maybe using the word-categorical feature’s concreteness can somehow determine the sentence type. And then using models to predict the probability of a comment being awarded a delta with the new features.
    </p>
    <p>
     Additionally, as mentioned in your report, the future direction could be how this interacts with other linguistic factors. I think in our research about topic modeling. We identified that different topics will have different preferences for delta replies. Maybe under the different topics, people will have different preferences of personal belief and experience.
    </p>
    <p>
     <break>
     </break>
    </p>
    <h3>
     ------------------------------------
    </h3>
    <div style="text-indent: 2em;">
     <h3>
      Author: Remy Stewart (student)
     </h3>
     <h3>
      Vote Count: 1
     </h3>
     <p>
      <b>
       A1 Reflection
      </b>
     </p>
     <p>
      1.) What lesson(s) would you like to pass on from your A1 experience?
     </p>
     <p>
      I think my largest takeaway from my A1 project is just how much NLP research overall benefits by intentionally considering complexity, awareness to nuance, and conditional situations for developing a research question as well as towards a given dataset. I know within quantitative analysis attempting to incorporate more features to predict a phenomenon can lead to concerns towards overfitting and p-hacking – which of course needs to be avoided within NLP work as well – but I think NLP topics often present a rather different circumstance compared to other research applications. Language is often more complex than what simple counts or yes/no indicators can adequately quantify. This is particularly true when you’re attempting to capture rather intricate linguistic tactics such as within my project regarding sharing a personal experience or delineating the reasoning behind one’s beliefs. This touches on Hovy and Yang’s emphasis on the fundamentally social nature of language and how much context, relationships, and beyond changes both how and why language is used in conversations. In the end, I’m not surprised that I didn’t find much of a relationship between the expression of experiences and/or beliefs towards persuasion in the CMV data. Conducting this research project reinforced for me that there are a lot more factors than just the number of occurrences of either of these rhetorical devices that impacts the outcome of successful or unsuccessful persuasion, which I strongly suspect is also applicable to various goals and intentions throughout conversations beyond CMV threads as well.
     </p>
     <p>
      2.) What are you most proud of?
     </p>
     <p>
      I’m most proud of the creativity of my project’s research idea through attempting to measure two complex rhetorical devices by letting the data itself guide me to relevant themes and key subjective takeaways. I appreciate my decision to employ skipgrams that were generated directly from the text to direct my analysis rather than just having my prior assumptions of how I would personally think a CMV commenter would present an experience or belief drive my variable operationalization alone. This bottom-up approach to measurement allowed me to explore new directions for model specifications that I wasn’t originally expecting to find. My two instances of this were creating a core versus ambiguous experience term list as well as adjusting for disproportionately occurring outliers. By actually reading through the comments themselves with particular attention towards the sentences where first-person pronouns lead to experience and belief terms, I was better able to understand the intended purposes behind each phrase and adjust my term lists accordingly. I’m proud that I let data exploration take the lead in my project instead of my own presumptions alone, which were quite often changed or refuted because
      <i>
      </i>
      I emphasized the emergent findings and therefore adjusted my research design accordingly.
     </p>
     <p>
      3.) What did you spend the most time on?
     </p>
     <p>
      The part of my pilot study I spent the most time on is related to what I’m the most proud of which was iterating on all the different lists of personal experience and personal belief terms I wanted to include as separate tests within my logistic regression models. This was driven by the skip grams for the most probable tokens associated with first-person pronouns and subsequentially reading through individual posts that my contender experience and belief phrases were featured in. This highlighted for me just how complex a given presentation of either rhetorical tactic could be beyond a simple binary of all instances of a phrase either referring to experiences or beliefs exclusively. Revising my working lists while reading through comments certainly took time, but it definitely led me to develop a more robust representation of how experiences and beliefs were being discussed within the CMV comments.
     </p>
     <p>
      It also took me extra time to conduct my outlier checks and create subsequent truncated term lists. I think in a lot of quantitative research outliers are often mishandled in two primary ways- they’re either ignored and left in the model with often undue influence on the findings, or they’re dropped as if they’re not also representative of an underlying trend in the data. Generating counts, producing diagnostic plots, and then comparing multiple versions of my logistic regressions with or without token outliers did inevitability make my research process longer. However, I’m glad I emphasized due diligence in discovering and subsequentially testing for the impact of these outliers rather than the faster routes of either ignoring them or dropping these words entirely.
     </p>
     <h3>
      ------------------------------------
     </h3>
    </div>
   </div>
  </div>
 </div>
 <h3>
  ----------- REPLIES -----------
 </h3>
</file>