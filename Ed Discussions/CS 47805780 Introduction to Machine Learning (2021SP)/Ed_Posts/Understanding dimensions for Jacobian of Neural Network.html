<h1>
 Title: Understanding dimensions for Jacobian of Neural Network
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-03T08:09:15.431625+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="131" src="https://static.us.edusercontent.com/files/cJLlRrykJZHyvsjHVmsRanun" width="441"/>
</figure>
<p>
 My first question has to do with the W_{d-l} matrix above. On the third line, why is it W_{d-l} instead of W_{d-l, 1}? Because in order for s_{d,1} to be scalar, don't we need a vector times a vector on the RHS, but here we have matrix times a vector giving the output as a vector?
</p>
<figure>
 <image height="105.75000000000001" src="https://static.us.edusercontent.com/files/khK3GZ8zEtLehvII33n5ZhtZ" width="658.0000000000001"/>
</figure>
<p>
 My second question has to do with the second picture. How were we able to go from
</p>
<math>
 J_{s_{d,1}}\left(v_{d,1}\right)
</math>
<p>
 on the first line to
</p>
<math>
 J_{s_d}\left(v_d\right)
</math>
<p>
 on the second? Isn't this changing the dimensions of the output?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Ji Won Kim (student)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  To answer your second question, I believe that the last layer of the NN only has one output (v_{d,1}) and one intermediate value (s_{d,1}), so the two are essentially equivalent (v_{d,1} == v_{d}) in terms of dimensions.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>