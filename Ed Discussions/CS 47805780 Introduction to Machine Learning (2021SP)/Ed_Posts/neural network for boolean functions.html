<h1>
 Title: neural network for boolean functions
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-09T03:35:10.726486+10:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="422" src="https://static.us.edusercontent.com/files/IaomO3yDiN9S5xzE79mI99Ie" width="644"/>
</figure>
<p>
 I don't quite understand how we know that, for boolean functions, we need at least exp(n) width? How did we get that the width would need to be exponential in n?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Michael Noor (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  The exp(n) corresponds to a brute-force enumeration of all possible combinations of input-output mappings
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Thorsten Joachims (admin)
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   ... and let me add that we did not prove that we need AT LEAST that many hidden units. But is it easy to see that we can represent any boolean function with that many hidden units. It is basically the same argument that we used when we talked about the expressive power of Kernel methods. Each element of the hidden layer can represent a conjunction, and then the output layer is a disjunctions. This way we can represent any boolean formula, since any boolean formula can be brought into disjunctive normal form.
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
</div>