<h1>
 Title: neural network for boolean functions
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-09T03:35:10.726486+10:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <figure>
  <image height="422" src="https://static.us.edusercontent.com/files/IaomO3yDiN9S5xzE79mI99Ie" width="644"/>
 </figure>
 <paragraph>
  I don't quite understand how we know that, for boolean functions, we need at least exp(n) width? How did we get that the width would need to be exponential in n?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Michael Noor (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  The exp(n) corresponds to a brute-force enumeration of all possible combinations of input-output mappings
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Thorsten Joachims (admin)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  ... and let me add that we did not prove that we need AT LEAST that many hidden units. But is it easy to see that we can represent any boolean function with that many hidden units. It is basically the same argument that we used when we talked about the expressive power of Kernel methods. Each element of the hidden layer can represent a conjunction, and then the output layer is a disjunctions. This way we can represent any boolean formula, since any boolean formula can be brought into disjunctive normal form.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>