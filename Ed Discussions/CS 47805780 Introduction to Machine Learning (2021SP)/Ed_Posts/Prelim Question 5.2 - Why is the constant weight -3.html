<h1>
 Title: Prelim Question 5.2 - Why is the constant weight -3?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-26T18:12:04.704283+10:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 In this problem, we gave the added constant feature a weight of -3. So then a positive example takes on a value of at least 5 - 3 = 2 (and is therefore always classified as positive), and a negative example takes on a value of at most 1 - 3 = -2 (and is therefore always classified as negative).
</p>
<p>
 Why couldn't we give the constant feature a weight of -2? Then positives would be 5 - 2 = 3 (positive), and negatives would be 1 - 2 = -1 (negative)... isn't that sufficient? What was the reason for choosing -3?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Oliver Leung (student)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  I think this is to provide the largest margin possible. When giving the constant feature a weight of -3, the functional margin is 2, but when it's -2, the functional margin is 1.
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Anonymous
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   I don't quite understand. Why is this the largest margin possible?
  </p>
  <h3>
   ------------------------------------
  </h3>
  <div style="text-indent: 6em;">
   <h3>
    Author: Oliver Leung (student)
   </h3>
   <h3>
    Vote Count: 1
   </h3>
   <p>
    Let's say that we set up $\vec{w}$ as described in the solution, without the constant weight - ones for all of the 20 relevant features, and zeroes for everything else. Then, we can think of the quantity $\vec{w} \cdot \vec{x}$ as a scalar on the number line from 0 to 20. If $\vec{x}$ is a positive example, then $\vec{w} \cdot \vec{x} \ge 5$, and if $\vec{x}$ is a negative example, then $\vec{w} \cdot \vec{x} \le 1$. Then, the easiest way to think of $\vec{w}$ is as a linear classifier in 1D (rather than in 80,000D).
   </p>
   <p>
    Without the constant weight, this classifier doesn't properly classify the negative examples. But, we know that there is a space of 4 in between the positive and negative examples, where none of the examples lie (and where we'd want to separate them). Divide that by 2, and we get that the largest possible functional margin we could ever achieve is 2, where the classifier is directly in between the positive and negative examples.
   </p>
   <h3>
    ------------------------------------
   </h3>
  </div>
 </div>
</div>