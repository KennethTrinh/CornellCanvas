<h1>
 Title: Prelim Question 5.2 - Why is the constant weight -3?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-26T18:12:04.704283+10:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  In this problem, we gave the added constant feature a weight of -3. So then a positive example takes on a value of at least 5 - 3 = 2 (and is therefore always classified as positive), and a negative example takes on a value of at most 1 - 3 = -2 (and is therefore always classified as negative).
 </paragraph>
 <paragraph>
  Why couldn't we give the constant feature a weight of -2? Then positives would be 5 - 2 = 3 (positive), and negatives would be 1 - 2 = -1 (negative)... isn't that sufficient? What was the reason for choosing -3?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Oliver Leung (student)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  I think this is to provide the largest margin possible. When giving the constant feature a weight of -3, the functional margin is 2, but when it's -2, the functional margin is 1.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Anonymous
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I don't quite understand. Why is this the largest margin possible?
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Oliver Leung (student)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  Let's say that we set up $\vec{w}$ as described in the solution, without the constant weight - ones for all of the 20 relevant features, and zeroes for everything else. Then, we can think of the quantity $\vec{w} \cdot \vec{x}$ as a scalar on the number line from 0 to 20. If $\vec{x}$ is a positive example, then $\vec{w} \cdot \vec{x} \ge 5$, and if $\vec{x}$ is a negative example, then $\vec{w} \cdot \vec{x} \le 1$. Then, the easiest way to think of $\vec{w}$ is as a linear classifier in 1D (rather than in 80,000D).
 </paragraph>
 <paragraph>
  Without the constant weight, this classifier doesn't properly classify the negative examples. But, we know that there is a space of 4 in between the positive and negative examples, where none of the examples lie (and where we'd want to separate them). Divide that by 2, and we get that the largest possible functional margin we could ever achieve is 2, where the classifier is directly in between the positive and negative examples.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>