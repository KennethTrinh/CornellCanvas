<h1>
 Title: Occam's Razor for Decision Trees
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-02-25T00:47:58.477114+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="488.16983240223465" src="https://static.us.edusercontent.com/files/DuCV1BwP7Oi2tTIKCJ0x6zXW" width="658"/>
</figure>
<p>
 <i>
  [From "Occam's Razor for Decision Trees" lecture]
 </i>
</p>
<p>
 The lecture mentioned "reduced-error tree pruning" as a post pruning method. My question is: could we also use the reduced-error criteria as an early stopping method? That is, instead of growing the whole tree then pruning, could we just initially stop splitting the tree if the drop in error is too low (like how we could do so with entropy) ?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Kaishuo Cheng (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  Yes, you can split a decision tree based on error or entropy, but entropy is more preferred.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>