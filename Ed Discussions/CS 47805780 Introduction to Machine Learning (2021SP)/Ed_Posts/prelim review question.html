<h1>
 Title: prelim review question
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-29T05:01:15.142642+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Is one reason that stochastic gradient descent is less sensitive to training data?
</p>
<figure>
 <image height="48.099415204678365" src="https://static.us.edusercontent.com/files/WgxZamjJqSHN4P8tYTT3rYbV" width="658"/>
</figure>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Archer Luo (student)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  I am not sure whether it helps with overfitting or not but one thing I know for sure is that stochastic gradient descent is faster than gradient descent and reduces computation cost at cost of a portion of theoretical guarantee.
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Jeremy Freedman (student)
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   Could anyone elaborate on the "theoretical guarantees" made by GD/SGD? I know when considering outputting the average of the weights computed vs. last iterate it impacts these guarantees. What are these guarantees being made? The lectures didn't go into much detail.
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
 <div style="text-indent: 2em;">
  <h3>
   Author: Scott Bass (student)
  </h3>
  <h3>
   Vote Count: 1
  </h3>
  <p>
   It can reduce overfitting because it's a noisy estimator of the gradient (so it's less dependent on the sample).
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
</div>