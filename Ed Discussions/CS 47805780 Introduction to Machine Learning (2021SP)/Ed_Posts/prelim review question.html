<h1>
 Title: prelim review question
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-29T05:01:15.142642+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Is one reason that stochastic gradient descent is less sensitive to training data?
 </paragraph>
 <figure>
  <image height="48.099415204678365" src="https://static.us.edusercontent.com/files/WgxZamjJqSHN4P8tYTT3rYbV" width="658"/>
 </figure>
 <paragraph>
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Scott Bass (student)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  It can reduce overfitting because it's a noisy estimator of the gradient (so it's less dependent on the sample).
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Archer Luo (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I am not sure whether it helps with overfitting or not but one thing I know for sure is that stochastic gradient descent is faster than gradient descent and reduces computation cost at cost of a portion of theoretical guarantee.
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Jeremy Freedman (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Could anyone elaborate on the "theoretical guarantees" made by GD/SGD? I know when considering outputting the average of the weights computed vs. last iterate it impacts these guarantees. What are these guarantees being made? The lectures didn't go into much detail.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>