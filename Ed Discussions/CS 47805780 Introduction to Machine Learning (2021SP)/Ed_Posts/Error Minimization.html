<h1>
 Title: Error Minimization
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-02-21T09:08:21.587177+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  When picking a feature that minimizes error, why don't we maximize $-\text{Err}(S|A)$ instead of $\text{Err}(S)-\text{Err}(S|A)$? Since the first term is always constant?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Haomiao Liu (staff)
</h3>
<h3>
 Vote Count: 2
</h3>
<document version="2.0">
 <paragraph>
  Computationally they are the same, but the subtraction just makes it clearer as to how much improvement of performance I get for this additional branching out.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>