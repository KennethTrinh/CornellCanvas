<h1>
 Title: Lecture 6b-perceptron
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-29T00:08:46.371323+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 "Any d-dimensional learning problem for non-homogenous linear classifiers has a homogenous form in (d+1) dimension." What is dimensional referring to? Is this the number of features in X, so if X had 3 features (color, firmness, texture), then we would could essentially add a fourth feature which would allow us to move to homogenous? How does that happen?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Victor Butoi (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  Yep :-)
 </p>
 <p>
  If you want to homogenize your linear classifier, then you would add a fourth feature to your x vector that is always one. That is because, if you dot your new x vector with the higher dimensional w vector, you get w1x1 + w2x2 + w3x3 + w4(1), where you can then interpret w4(1) as the bias since it isn't a function of x.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>