<h1>
 Title: bias-variance tradeoff
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-10T02:04:34.297876+10:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="424" src="https://static.us.edusercontent.com/files/CdrPLGfVcUt15GSia0ra9g87" width="645"/>
</figure>
<p>
 Regarding above where it indicates the bias-variance tradeoff, I just want to make sure I understand -- are we reducing variance because we can't approximate all functions but then reducing bias when we reduce the chance of overfitting?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Nils Bjorck (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  the idea is that deeper network, that are more expressive, suffer from higher variance as they can overfit. On the other hand, smaller networks might be biased as they can't represent all functions.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>