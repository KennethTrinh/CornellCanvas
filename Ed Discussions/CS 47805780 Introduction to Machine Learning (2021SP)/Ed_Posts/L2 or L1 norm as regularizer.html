<h1>
 Title: L2 or L1 norm as regularizer
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-27T16:30:17.242576+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 One of the practice exam q's is why someone would choose L2 over L1 regularization. Does L2 more strongly punish over-fitting?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Anna Effenberger (student)
 </h3>
 <h3>
  Vote Count: 4
 </h3>
 <p>
  the L1 norm is the sum of |w_i|, while L2 norm is the square-root of the sum of w_i^2.
 </p>
 <p>
  So L1 enforces a sparsity, as it just looks for a few important features to put a lot of weight on.
 </p>
 <p>
  L2 is smoother and allocates weights more evenly across the different features, since it favors a w with low norm.
 </p>
 <p>
  There was a demo of this in this week's discussion
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>