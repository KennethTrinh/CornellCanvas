<h1>
 Title: When to prefer L1 regularization and when to prefer L2 ?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-28T05:11:05.392801+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 When to prefer L1 regularization and when to prefer L2 over the other?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Artem Streltsov (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  L2 prevents overfitting by controlling size of the weights (all weights are small and of positive magnitude); L1 instead pushes some weights all the way to 0 and essentially carries out a feature selection. so the question is whether you want to completely rule out some features or keep using them all and just want to prevent overfitting. computationally l2 is also quite a bit easier, although there are good algos out there for lasso now too.
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Anonymous
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   So, to summarize, can I write
  </p>
  <p>
   L1 is to be preferred for feature selection  and
  </p>
  <p>
   L2 is to be preferred to avoid overfitting
  </p>
  <h3>
   ------------------------------------
  </h3>
  <div style="text-indent: 6em;">
   <h3>
    Author: Artem Streltsov (staff)
   </h3>
   <h3>
    Vote Count: 1
   </h3>
   <p>
    yes. for the record, l1 reduces overfitting too. wlog dropping poor features reduces overfitting
   </p>
   <h3>
    ------------------------------------
   </h3>
  </div>
 </div>
</div>