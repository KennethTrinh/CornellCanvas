<h1>
 Title: At least log_2(n) mistakes
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-21T21:54:15.183757+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 2
</h3>
<document version="2.0">
 <paragraph>
  Why does this slide say that the algorithm makes
  <bold>
   at least
  </bold>
  $log_2(n)$ mistakes. I thought that since you make at most depth of the tree # mistakes, before you reach a consistent hypothesis, you make
  <bold>
   at most
  </bold>
  $log_2(n)$ mistakes.
 </paragraph>
 <figure>
  <image height="487.09090909090907" src="https://static.us.edusercontent.com/files/aozG3kSvWmRFa2bDQJFSg6tM" width="658"/>
 </figure>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Maitreyi Chatterjee (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I think this is specific to this 1D threshold hypothesis . We make you make
  <bold>
   at most
  </bold>
  log_2(n)log2â€‹(n) mistakes.
 </paragraph>
 <paragraph>
  for the example of the 1D threshold, but other hypothesises are possible that make more mistakes.
 </paragraph>
 <paragraph>
  So the best mistake is log2(n)
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Anonymous
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I don't follow. It seems that you are also arguing that we make at most $log_2(n)$ mistakes when the slide says that we make at least $log_2(n)$ mistakes.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Maitreyi Chatterjee (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  That is for this algorithm and the at least is for all algos in general?
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Yimeng Min (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  My understanding is that for any h_i, there exists a data distribution that will have at least log2n mistakes.
 </paragraph>
 <paragraph>
  The data distribution can be easily constructed by reversing the labels along the tree. That's why you get the log2n bound.
 </paragraph>
 <paragraph>
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>