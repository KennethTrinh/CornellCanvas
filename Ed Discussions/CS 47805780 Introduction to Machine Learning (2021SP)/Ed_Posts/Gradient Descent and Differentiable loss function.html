<h1>
 Title: Gradient Descent and Differentiable loss function
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-29T12:53:57.755266+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 In the practice prelim, a question asks T/F "for gradient descent to converge, the loss function must be convex and differentiable everywhere". This is false, right? Because in lecture we've seen an example of GD with hinge loss, which is not differentiable at one point.
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Woosang Kang (student)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  That could be one way to reason, but one other fundamental reason could be that the optimization that we're doing in GD is not necessarily convex, possibly converging in a local minimum instead of a single, global minimum (which is one of the reasons why we use SGD).
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>