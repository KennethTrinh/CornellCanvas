<h1>
 Title: "Best" Linear Classifier and Margin Intuition
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-04T03:36:35.913592+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Since we have so many hyperplanes separating for example, two classes of data, are they all equally as desirable; if not, how do we choose the "best" one, especially since the hyperplane produced by the Perceptron changes with shuffling the order of the training data...
</p>
<p>
 Also, I wanted to check whether the margin with respect to a training data set and a hyperplane, assuming we have misclassified examples, is a negative value with the largest magnitude represented by the distance from the hyperplane to the misclassified example, since we optimize $\gamma$; $\gamma = min_{i \in S} \{ y_i(\vec{w}^* \cdot \vec{x_i})\}$.
</p>
<p>
 Thanks!
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Jane Du (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  A great question, and one that is absolutely coming up in the next few lectures, when you learn about SVMs!
  <break>
  </break>
  In a (loose) nutshell, the "best" one is the one that is as far as possible away from the points of the two classes it's separating -- aka, has the best margin. Again, this is covered in detail soon.
  <break>
  </break>
  <break>
  </break>
  Yes, the margin on a hyperplane that is currently misclassifying examples should have a negative margin. We don't really talk about it too much for perceptron, since the algorithm won't even converge, but your interpretation is correct!
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Anonymous
  </h3>
  <h3>
   Vote Count: 1
  </h3>
  <p>
   Thanks so much for the quick reply!!
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
</div>