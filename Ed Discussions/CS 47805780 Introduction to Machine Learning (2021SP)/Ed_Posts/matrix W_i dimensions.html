<h1>
 Title: matrix W_i dimensions
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-07T01:18:46.073381+10:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Is it usually the case that the hidden layers in a neural network all have the same # of nodes, or is it common that they have differing # of nodes?
</p>
<p>
 That is, is it usually the case that the matrix W_i representing the weights between two layers is square, or is it common for this matrix to be non-square in dimensions?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Raahi Menon (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  They don't need to have the same number of nodes. It's a lot easier to do math when they do, but as we've seen in convolutional networks a lot of times changing the size of the input between layers can make the network much more efficient.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>