<h1>
 Title: matrix W_i dimensions
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-07T01:18:46.073381+10:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Is it usually the case that the hidden layers in a neural network all have the same # of nodes, or is it common that they have differing # of nodes?
 </paragraph>
 <paragraph>
  That is, is it usually the case that the matrix W_i representing the weights between two layers is square, or is it common for this matrix to be non-square in dimensions?
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Raahi Menon (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  They don't need to have the same number of nodes. It's a lot easier to do math when they do, but as we've seen in convolutional networks a lot of times changing the size of the input between layers can make the network much more efficient.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>