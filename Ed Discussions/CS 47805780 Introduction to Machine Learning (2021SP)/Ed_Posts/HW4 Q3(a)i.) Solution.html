<h1>
 Title: HW4 Q3(a)i.) Solution
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-26T13:00:41.49+10:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 In Q3(a), the setup is a fully connected network with a flattened input of size $D$, a hidden layer of size $D$, and output of size $C$.
</p>
<p>
 Why is the number of parameters from the input to the hidden layer
 <b>
  with bias
 </b>
 just $D^2$? I thought it would be $(D+1)D$, with $+ 1$ to account for weights going from the bias to each of the hidden layer nodes.
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Victor Butoi (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  I think you are right, with a linear layer that goes from D to D, without bias is D^2 and with bias is D^2 + D. If you lost points on this please ping one of the TAs.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>