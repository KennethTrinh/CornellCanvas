<h1>
 Title: Lecture 6a_perceptron projections
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-29T00:02:43.680914+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I'm confused by the statement "Projection and dot products: For any vector and any vector w normalized to have ∥w∥=1, w x =xcos(theta)is the signed length of the projection of x on w."
 </paragraph>
 <paragraph>
  Why is this important? Is this saying that when we have a linear classification problem with a normalized weight vector and a feature matrix X, that we are concerned with the length of x onto w because that is telling us how far away x is from our w? So by grabbing the signed length, we know which direction the vector is going?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Victor Butoi (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="1.0">
 <paragraph>
  Close, in the case of svms we are interested with the actual distance to our hyper plane, thus this signed distance becomes very useful.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>