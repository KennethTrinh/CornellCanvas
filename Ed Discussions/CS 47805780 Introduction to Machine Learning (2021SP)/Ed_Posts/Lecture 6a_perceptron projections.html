<h1>
 Title: Lecture 6a_perceptron projections
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-29T00:02:43.680914+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 I'm confused by the statement "Projection and dot products: For any vector and any vector w normalized to have ∥w∥=1, w x =xcos(theta)is the signed length of the projection of x on w."
</p>
<p>
 Why is this important? Is this saying that when we have a linear classification problem with a normalized weight vector and a feature matrix X, that we are concerned with the length of x onto w because that is telling us how far away x is from our w? So by grabbing the signed length, we know which direction the vector is going?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Victor Butoi (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  Close, in the case of svms we are interested with the actual distance to our hyper plane, thus this signed distance becomes very useful.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>