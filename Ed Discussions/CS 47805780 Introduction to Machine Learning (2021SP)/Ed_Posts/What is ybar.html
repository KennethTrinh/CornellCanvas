<h1>
 Title: What is ybar
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-28T03:18:35.426306+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  What is ybar mean in the slides on different commonly used loss functions? Is it w dot x or w dot x + b? For reference
 </paragraph>
 <figure>
  <image height="361.6163793103448" src="https://static.us.edusercontent.com/files/a9oAYzbhoHFM8v9aN63G15JT" width="658"/>
 </figure>
 <paragraph>
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Atul Ganju (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  ybar means the classification given to the training example by the classifier. Homogenous perceptron for example would give a classification that runs through the origin so there would be no bias term. Non-homogenous perceptron however may have a bias term at the end of its classification.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>