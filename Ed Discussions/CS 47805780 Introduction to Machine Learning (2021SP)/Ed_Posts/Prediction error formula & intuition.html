<h1>
 Title: Prediction error formula &amp; intuition
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-02-24T01:23:32.831891+11:00
</h3>
<h3>
 Category: Lectures
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="148" src="https://static.us.edusercontent.com/files/cXtemVlFm97jiownkzzH1Avs" width="502"/>
</figure>
<p>
 [From "Learning as Prediction" lecture]
</p>
<p>
 I don't quite understand how this formula represents prediction error, and how to understand prediction error intuitively.
</p>
<list style="number">
 <li>
  <p>
   Why can we represent prediction error as the expected value of the error function {delta(h(X),Y)} ? How is this equivalent to the sum of the error function between h(x) and y, weighted by the joint probability of X=x and Y=y?
  </p>
 </li>
 <li>
  <p>
   I understand that prediction error has to do with the difference between {what the true label of an instance is in the real world} and {the label that our hypothesis h predicts}, but, from there, I'm not sure how to exactly understand prediction error.
  </p>
 </li>
</list>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Thorsten Joachims (admin)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  You can think of P(X,Y) as a big bag of examples from which we draw with replacement. Note that there can be duplicates in the bag, so that not all specific (x,y) are drawn with the same probability.
 </p>
 <p>
  The prediction error models the following process: draw a (x,y) from the bag P(X,Y), compute the prediction $\hat{y}=h(x)$, and then compare $\hat{y}$ against the true y (via $\Delta$). The prediction error $err_P(h)$ is the expected fraction of errors that you are making in this process. For the zero-one loss, this is equal to the probability of drawing an example (x,y) from P(X,Y) for which true label y and predicted label $\hat{y}$ disagree.
 </p>
 <p>
  Going from from the expectation $E_{(X,Y)}(\Delta(h(X),Y))$ to $\sum_{(x,y)}  \Delta(h(x),x) P(X=x,Y=y)$ is just plugging in the definition of expectation for discrete random variables (for continuous random variables this would be an integral, but let's skip the gratuitous math that comes with that).
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>