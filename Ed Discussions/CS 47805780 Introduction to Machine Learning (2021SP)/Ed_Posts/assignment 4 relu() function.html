<h1>
 Title: assignment 4 relu() function
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-05T11:51:35.561555+10:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Q1 says to create a layer of relu activation function of less than 4 nodes, does that mean all four nodes should use the same relu function?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Jane Du (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  Good question, and one that we underspecified.
 </p>
 <p>
  There exists at least one solution using at most 4 hidden nodes using ReLU. Other activation functions like sigmoid are not likely to be useful here.
 </p>
 <p>
  You can, however, use a node like the identity node, that just outputs what it receives as input. But please make sure to label this very clearly in your solution, as we will typically assume hidden nodes are ReLU by default.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>