<h1>
 Title: SVM optimization problem question
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-13T00:34:32.917709+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<figure>
 <image height="232.65185185185183" src="https://static.us.edusercontent.com/files/CToNkPYcM5W9UO1IANZWqDn2" width="302"/>
</figure>
<p>
 In lecture, it was mentioned that the hard-margin SVM optimization problem is a convex quadratic optimization problem that can be solved with "a ball rolling downhill" (i.e. gradient descent) -- I don't quite understand what this is referring to, and I haven't really learned about gradient descent before. Will we be talking more about this later? Are there any resources I could read about this more for now so that I can better understand how we're solving this optimization problem?
</p>
<p>
 Also, what is meant by a "convex" optimization problem? How is this different from non-convex optimization problems?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Samar Khanna (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  Stay tuned for gradient descent, it will be coming up shortly! You could read ahead in the UML textbook about Gradient Descent (chapter 14 in my version).
 </p>
 <p>
  One of the properties of convex optimisation problems is that local solutions are also globally optimal. (Think: finding the minimum of a parabola like $x^2 + x + 2$). Convex problems are generally "feasible" to solve efficiently- general mathematical optimisation is NP-Hard.
 </p>
 <p>
  For a lot more intuition and detail, check out chapter 12 of the UML textbook on "Convex Learning Problems" (again, the chapter number might be different in my version). If you have any questions, feel free to follow up :)
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>