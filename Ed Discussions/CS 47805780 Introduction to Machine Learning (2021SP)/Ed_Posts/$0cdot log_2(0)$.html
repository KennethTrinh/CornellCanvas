<h1>
 Title: $0\cdot \log_2(0)$?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-02-19T10:00:51.184921+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  When doing homework, I encountered the case when calculating the entropy when all data points have the same label, and I get $H(S) = -1\cdot \log_2(1) - 0 \cdot \log_2(0)$, where the second term is basically $-0 \cdot \infty$. Do I just assume this term is zero?? Is the entropy simply zero then since there is no possible variations?
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Abhay Singh (staff)
</h3>
<h3>
 Vote Count: 3
</h3>
<document version="2.0">
 <paragraph>
  Yeah, you can assume that is 0.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>