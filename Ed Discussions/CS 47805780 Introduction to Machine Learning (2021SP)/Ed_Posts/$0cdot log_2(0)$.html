<h1>
 Title: $0\cdot \log_2(0)$?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-02-19T10:00:51.184921+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 When doing homework, I encountered the case when calculating the entropy when all data points have the same label, and I get $H(S) = -1\cdot \log_2(1) - 0 \cdot \log_2(0)$, where the second term is basically $-0 \cdot \infty$. Do I just assume this term is zero?? Is the entropy simply zero then since there is no possible variations?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Abhay Singh (staff)
 </h3>
 <h3>
  Vote Count: 3
 </h3>
 <p>
  Yeah, you can assume that is 0.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>