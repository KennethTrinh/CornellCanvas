<h1>
 Title: Gradient Descent Converging
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-29T12:47:24.299591+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 What does it mean for gradient descent to converge? Does this mean converge to a global minimum, or simply that it takes finite time to produce an answer?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Victor Butoi (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  When we say that gradient descent converges, we are typically talking in the ERM (Empirical risk minimization setting) with some loss that we the gradient of to make our weight updates. In this case, we are referring to  the value of the loss converging to a noise ball.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>