<h1>
 Title: Gradient Descent Converging
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-29T12:47:24.299591+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  What does it mean for gradient descent to converge? Does this mean converge to a global minimum, or simply that it takes finite time to produce an answer?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Victor Butoi (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  When we say that gradient descent converges, we are typically talking in the ERM (Empirical risk minimization setting) with some loss that we the gradient of to make our weight updates. In this case, we are referring to  the value of the loss converging to a noise ball.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>