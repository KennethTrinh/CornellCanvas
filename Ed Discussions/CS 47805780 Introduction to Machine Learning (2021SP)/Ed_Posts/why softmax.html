<h1>
 Title: why softmax
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-05-11T03:39:04.511148+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 1
</h3>
<p>
 softmax casts scores into probabilities to cast it to a probability distribution.
</p>
<p>
 [1, 2] -&gt; [$(e^1/(e^1+e^2)$, $(e^2/(e^1+e^2)$]
</p>
<p>
 But we could have just used standard normalization to cast it to a probability distribution.
</p>
<p>
 [1, 2] -&gt; [$1/(1+2)$, $2/(1+2)$]
</p>
<p>
 So why softmax?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Michael Noor (staff)
 </h3>
 <h3>
  Vote Count: 2
 </h3>
 <p>
  One primary reason we use softmax is that it tolerates negative values well/logically.
 </p>
 <p>
  Say, for instance, we have [1, -2]. Softmax will output:
 </p>
 <math>
  \left[\frac{e}{e+e^{-2}},\ \frac{e^{-2}}{e+e^{-2}}\right]\approx\left[.95,.05\right]
 </math>
 <p>
  Whereas standard normalization would output:
 </p>
 <math>
  \left[\frac{1}{1-2},\frac{-2}{1-2}\right]=\left[-1,2\right]
 </math>
 <p>
  Both sum up to one, but softmax outputs probabilities and standard normalization does not.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>