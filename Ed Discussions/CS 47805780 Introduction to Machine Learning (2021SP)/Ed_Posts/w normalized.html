<h1>
 Title: w normalized?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-03T14:26:02.821493+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Are we supposed to normalize the weight vector w and alter b accordingly at the end of the perceptron run?
</p>
<p>
 I ask because the convergence proofs always mentioned unit norm w, but the algorithms don't mention it.
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Sanjali Jha (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  You don't have to normalize the weight vector or b at each perceptron iteration. The convergence proof uses a specific unit length w (and its norm) to prove by contradiction.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>