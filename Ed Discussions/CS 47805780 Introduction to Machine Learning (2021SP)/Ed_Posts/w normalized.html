<h1>
 Title: w normalized?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-03T14:26:02.821493+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Are we supposed to normalize the weight vector w and alter b accordingly at the end of the perceptron run?
 </paragraph>
 <paragraph>
  I ask because the convergence proofs always mentioned unit norm w, but the algorithms don't mention it.
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Sanjali Jha (staff)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  You don't have to normalize the weight vector or b at each perceptron iteration. The convergence proof uses a specific unit length w (and its norm) to prove by contradiction.
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>