<h1>
 Title: Neural Network Convergence
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-07T05:04:51.541598+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Hi,
</p>
<p>
 I've got the following question regarding neural networks: Say we have two neural networks, A and B, that are both trained on a sample data set $S$. If network A is known to converge (all its weights are initialized to zero), and if we bring in another network identical to network A, say network B, whose weights are initialized to a weight matrix from an arbitrary intermediate step in the process of training network A, then are there any guarantees of convergence for network B?
</p>
<p>
 Thank you!
</p>
<div style="text-indent: 2em;">
 <h3>
  Author: Atul Ganju (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  Assuming you are using the same loss function, and minimizing it in the same way, it will converge.
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Anonymous
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   Thank you! I was just wondering if I was clear enough with my description; by
   <i>
    intermediate step
   </i>
   , I meant to assume that the weight matrix is continually updated after every individual weight update of backprop. In particular, I was wondering if there were any guarantees for the case when the weights of Network B could be initialized to the weight matrix at a state when Network A is in the middle of performing backprop...
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
</div>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Michael Noor (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  Yes, gradient descent on the full dataset is deterministic, so the updates on B will be identical to the ones on A.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>