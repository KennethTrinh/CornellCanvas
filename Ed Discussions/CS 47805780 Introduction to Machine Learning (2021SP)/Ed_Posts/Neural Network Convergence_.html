<h1>
 Title: Neural Network Convergence
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-07T05:04:51.541598+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Hi,
 </paragraph>
 <paragraph>
  I've got the following question regarding neural networks: Say we have two neural networks, A and B, that are both trained on a sample data set $S$. If network A is known to converge (all its weights are initialized to zero), and if we bring in another network identical to network A, say network B, whose weights are initialized to a weight matrix from an arbitrary intermediate step in the process of training network A, then are there any guarantees of convergence for network B?
 </paragraph>
 <paragraph>
  Thank you!
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 Author: Atul Ganju (staff)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  Assuming you are using the same loss function, and minimizing it in the same way, it will converge.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Anonymous
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Thank you! I was just wondering if I was clear enough with my description; by
  <italic>
   intermediate step
  </italic>
  , I meant to assume that the weight matrix is continually updated after every individual weight update of backprop. In particular, I was wondering if there were any guarantees for the case when the weights of Network B could be initialized to the weight matrix at a state when Network A is in the middle of performing backprop...
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Michael Noor (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Yes, gradient descent on the full dataset is deterministic, so the updates on B will be identical to the ones on A.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>