<h1>
 Title: Kernel Definition
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-03-29T08:21:16.915647+11:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  So the kernel is defined as some inner product applied after some transformation (typically to a higher dimension). However, many examples (ex: some questions in the hmwk) show us kernels defined simply by the dot product. During the exam, is it alright to do this and write the kernel as \phi \cdot \phi, or should we write the kernel using inner product notation &lt;\phi, \phi&gt;?
 </paragraph>
 <paragraph>
  Also, why is the kernel a similarity measure? I understand how that is true for the dot product, but I can think of inner products that aren't necessarily measures of similarity.
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Atul Ganju (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  The dot product is an example of an inner product. Furthermore, for this class, we only ever explicitly deal with inner products that are dot products. To see why inner products are used in machine learning, the first answer in this stack exchange post explains it quite nicely:
  <link href="https://math.stackexchange.com/questions/1219106/what-is-the-rigorous-justification-for-using-inner-products-as-a-function-of-sim"/>
  link :)
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>