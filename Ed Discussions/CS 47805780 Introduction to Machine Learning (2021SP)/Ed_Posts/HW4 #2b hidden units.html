<h1>
 Title: HW4 #2b hidden units
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-02T13:20:54.132535+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 8
</h3>
<document version="2.0">
 <paragraph>
  For the min/max networks, is it ok if one of the hidden units is not a ReLU activation but is just a linear combination of the inputs? Or do all hidden units have to be a ReLU activation?
 </paragraph>
</document>
<h3>
 Author: Ji Won Kim (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  +1 I'm curious about this as well.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Sunwook Kim (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  I'm not sure which linear combination it is, but I had a linear combination that ended up being representable as a sum of ReLU functions. (I had a linear unit that ended up being represented by the sum of two ReLU values)
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Amelia Myers (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  A TA answered this question a few hours ago in this post: #841
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>