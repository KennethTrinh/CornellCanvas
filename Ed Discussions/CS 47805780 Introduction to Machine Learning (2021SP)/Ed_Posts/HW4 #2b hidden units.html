<h1>
 Title: HW4 #2b hidden units
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-02T13:20:54.132535+11:00
</h3>
<h3>
 Category: Problem Sets
</h3>
<h3>
 Vote Count: 8
</h3>
<p>
 For the min/max networks, is it ok if one of the hidden units is not a ReLU activation but is just a linear combination of the inputs? Or do all hidden units have to be a ReLU activation?
</p>
<div style="text-indent: 2em;">
 <h3>
  Author: Ji Won Kim (student)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  +1 I'm curious about this as well.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Amelia Myers (student)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  A TA answered this question a few hours ago in this post: #841
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 2em;">
  <h3>
   Author: Sunwook Kim (student)
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   I'm not sure which linear combination it is, but I had a linear combination that ended up being representable as a sum of ReLU functions. (I had a linear unit that ended up being represented by the sum of two ReLU values)
  </p>
  <h3>
   ------------------------------------
  </h3>
 </div>
</div>