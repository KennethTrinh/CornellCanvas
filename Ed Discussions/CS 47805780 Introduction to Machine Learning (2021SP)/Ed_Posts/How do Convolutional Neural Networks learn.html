<h1>
 Title: How do Convolutional Neural Networks learn?
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-04-20T15:24:53.971179+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 1
</h3>
<p>
 We learned to construct gradients for weights in flattened neural networks, but how do we do backpropagation in CNN's? I assume the Jacobian method we learned wouldn't work since weight matrices would now be 3d (or even 4d)?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Victor Butoi (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  This is such a great question.
 </p>
 <p>
  The tldr; we update the filter weights by convolving with a rotated version of the filter and the loss gradient from the next layer.
 </p>
 <p>
  The non tldr is a lot of math, but if you're interested I recommend:
  <a href="https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c">
   https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c
  </a>
 </p>
 <p>
  P.S. In general, whenever I'm confused about something in ML, I've always found that when I google "X medium" there is a great visual explanation.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>