<h1>
 Title: tanh vs relu
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-05-11T03:42:19.735561+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  When should I use tanh and when should I use relu activation functions? Additionally, why does the industry strongly lean towards the RELU activation function?
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Michael Noor (staff)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  Tanh and ReLU both have different limits on their range, where Tanh(x) ∈ (-1,1) and ReLU ∈ [0, ∞). Industry tends to lean toward ReLU because of the wider range of expressibility, and the clean derivative (which makes it computationally cheaper). To actually know which is better for your application, you should probably just try them.
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>