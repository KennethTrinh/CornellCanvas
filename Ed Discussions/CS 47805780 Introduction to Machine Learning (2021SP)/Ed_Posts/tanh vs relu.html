<h1>
 Title: tanh vs relu
</h1>
<h3>
 Author: Anonymous
</h3>
<h3>
 Date: 2021-05-11T03:42:19.735561+10:00
</h3>
<h3>
 Category: General
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 When should I use tanh and when should I use relu activation functions? Additionally, why does the industry strongly lean towards the RELU activation function?
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Michael Noor (staff)
 </h3>
 <h3>
  Vote Count: 1
 </h3>
 <p>
  Tanh and ReLU both have different limits on their range, where Tanh(x) ∈ (-1,1) and ReLU ∈ [0, ∞). Industry tends to lean toward ReLU because of the wider range of expressibility, and the clean derivative (which makes it computationally cheaper). To actually know which is better for your application, you should probably just try them.
 </p>
 <h3>
  ------------------------------------
 </h3>
</div>