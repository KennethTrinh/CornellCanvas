<h1>
 Title: Additional Prelim Practice Question 2
</h1>
<h3>
 Author: Elena Murguia (student)
</h3>
<h3>
 Date: 2023-03-29T02:58:28.62353+11:00
</h3>
<h3>
 Category: Prelim
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Could someone further explain the answers to 2a and 2b on the additional practice prelim questions? I don't understand why the mean is just the coefficient in from of the value from the dataframe. Thanks!
 </paragraph>
</document>
<h3>
 ----------- REPLIES -----------
</h3>
<h3>
 Author: Yuheng Wang (staff)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  One thing you can first think about is, what if we remove the last term 0.2*np.random.randn(n). In this case, the code is doing a linear regression between y and x1 &amp; x2, with no additional noise. And we know that the true coefficient will result in the minimization of the squared loss, which indicates that the sm.OLS will return -2 &amp; 4 for all x1_vex and x2_vec.
 </paragraph>
 <paragraph>
  If you understand this, then consider the case when there are some random noise added to the model. Notice that the noise 0.2*np.random.randn(n) has a mean of 0 (check the documentation of np.random.randn() if you are not sure). So on average, we should expect that OLS will predict the true model coefficient well.
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Jonathan Gotian (student)
</h3>
<h3>
 Vote Count: 0
</h3>
<document version="2.0">
 <paragraph>
  Can you please elaborate on "And we know that the true coefficient will result in the minimization of the squared loss, which indicates that the sm.OLS will return -2 &amp; 4 for all x1_vex and x2_vec."
 </paragraph>
 <paragraph>
  If np.random.randn has mean 0 then why does multiply by anything not produce 0?
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>
<h3>
 Author: Yuheng Wang (staff)
</h3>
<h3>
 Vote Count: 1
</h3>
<document version="2.0">
 <paragraph>
  x1 is generated by np.random.randn(), which has a mean 0. But the correlation between y and x1 is not 0. Consider this example: you generate x1 to be [1, -1]. and x2 to be [2, -2]. Based on the formula for y = 3 - 2 * x1 + 4*x2, you can compute y = [9, -3]. Specifically:
 </paragraph>
 <paragraph>
  y     |    x1       |    x2
 </paragraph>
 <paragraph>
  -------------------------------
 </paragraph>
 <paragraph>
  9     |   1           |    2
 </paragraph>
 <paragraph>
  -3    |  -1           |  -2
 </paragraph>
 <paragraph>
 </paragraph>
 <paragraph>
  Notice that if you were to fit a linear regression on this model, you could find the coefficient before x1 to be precisely -2, and coefficient before x2 to be precisely 4. Since the linear regression minimizes the square loss and the value -2 and 4 (and also with the constant offset) could make the squared loss to 0.
 </paragraph>
 <paragraph>
 </paragraph>
</document>
<h3>
 ------------------------------------
</h3>