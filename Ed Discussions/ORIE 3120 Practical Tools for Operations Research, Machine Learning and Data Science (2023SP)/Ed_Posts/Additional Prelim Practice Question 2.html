<h1>
 Title: Additional Prelim Practice Question 2
</h1>
<h3>
 Author: Elena Murguia (student)
</h3>
<h3>
 Date: 2023-03-29T02:58:28.62353+11:00
</h3>
<h3>
 Category: Prelim
</h3>
<h3>
 Vote Count: 0
</h3>
<p>
 Could someone further explain the answers to 2a and 2b on the additional practice prelim questions? I don't understand why the mean is just the coefficient in from of the value from the dataframe. Thanks!
</p>
<h3>
 ----------- REPLIES -----------
</h3>
<div style="text-indent: 2em;">
 <h3>
  Author: Yuheng Wang (staff)
 </h3>
 <h3>
  Vote Count: 0
 </h3>
 <p>
  One thing you can first think about is, what if we remove the last term 0.2*np.random.randn(n). In this case, the code is doing a linear regression between y and x1 &amp; x2, with no additional noise. And we know that the true coefficient will result in the minimization of the squared loss, which indicates that the sm.OLS will return -2 &amp; 4 for all x1_vex and x2_vec.
 </p>
 <p>
  If you understand this, then consider the case when there are some random noise added to the model. Notice that the noise 0.2*np.random.randn(n) has a mean of 0 (check the documentation of np.random.randn() if you are not sure). So on average, we should expect that OLS will predict the true model coefficient well.
 </p>
 <h3>
  ------------------------------------
 </h3>
 <div style="text-indent: 4em;">
  <h3>
   Author: Jonathan Gotian (student)
  </h3>
  <h3>
   Vote Count: 0
  </h3>
  <p>
   Can you please elaborate on "And we know that the true coefficient will result in the minimization of the squared loss, which indicates that the sm.OLS will return -2 &amp; 4 for all x1_vex and x2_vec."
  </p>
  <p>
   If np.random.randn has mean 0 then why does multiply by anything not produce 0?
  </p>
  <h3>
   ------------------------------------
  </h3>
  <div style="text-indent: 6em;">
   <h3>
    Author: Yuheng Wang (staff)
   </h3>
   <h3>
    Vote Count: 1
   </h3>
   <p>
    x1 is generated by np.random.randn(), which has a mean 0. But the correlation between y and x1 is not 0. Consider this example: you generate x1 to be [1, -1]. and x2 to be [2, -2]. Based on the formula for y = 3 - 2 * x1 + 4*x2, you can compute y = [9, -3]. Specifically:
   </p>
   <p>
    y     |    x1       |    x2
   </p>
   <p>
    -------------------------------
   </p>
   <p>
    9     |   1           |    2
   </p>
   <p>
    -3    |  -1           |  -2
   </p>
   <p>
    Notice that if you were to fit a linear regression on this model, you could find the coefficient before x1 to be precisely -2, and coefficient before x2 to be precisely 4. Since the linear regression minimizes the square loss and the value -2 and 4 (and also with the constant offset) could make the squared loss to 0.
   </p>
   <h3>
    ------------------------------------
   </h3>
  </div>
 </div>
</div>